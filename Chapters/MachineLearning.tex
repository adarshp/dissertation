\chapter{Machine Learning in Particle Physics}\label{ch:MachineLearning}
\section{Particle physics as a classification problem}
Separating signals and backgrounds in particle physics is, in the language of machine learning, a \emph{classification} problem. One can consider particle collision events as living in a space, where the coordinates of each event are represented by various \emph{features} of the event. These features could be high-level ones such as razor variables, or low-level ones, such as the momentum components of individual final state particles. What we do when we perform a cut-and-count analysis is to try and isolate a region of this space that is rich in signal events. Typically, the most straightforward approach is to define a sort of `box' in feature space using rectangular, one-dimensional cuts. However, there is no guarantee that signal events are confined to such a box --- correlations between the features can distort the distribution of events in feature space. 
\strictpagecheck
\begin{figure}
  \begin{sidecaption}{Illustration of a non-linear decision boundary in two-dimensional feature space.}
  \input{code/scatter.pgf}
\end{sidecaption}
\label{fig:nonlinear_decision_boundary}
\end{figure}
If we consider a two- or three- dimensional feature space, this distribution of the events is typically easy to visualize, and can often be explained through a simple kinematical relation between the features. Thus, instead of a box, we might draw a ring, or perhaps some other shape as a `lasso' around the events. This kind of delineation between signal and background events is known in machine learning terms as a \emph{decision boundary}. As we go to higher dimensions, manually choosing this becomes virtually impossible - instead we resort to looking at one- and two-dimensional slices of the space and imposing cuts on those. 

However, these cuts may fail to fully capture the potentially complicated correlations among the features, and we might end up rejecting too many signal events, or accepting too many background events. Take for example the distribution of data from a toy experiment in \autoref{fig:nonlinear_decision_boundary}. The decision boundary that separates the signal events (blue) and the background events (red) is curved. Applying rectilinear cuts in this feature space would clearly be suboptimal. Machine learning techniques (or, as they are sometimes referred to in the particle physics literature, multivariate techniques) can harness the correlations between features more efficiently than manually attempting to find the optimal decision boundary in feature space.

In the context of particle physics, a \emph{binary classifier} is a function that takes as input various features of an event, and outputs a score for the event corresponding to whether it is more signal-like or more background-like. The form of the function itself depends on a number of parameters, which are \emph{a priori} unknown. In \emph{supervised learning} (which encompasses the most commonly-used ML techniques), these parameters are set by feeding the classifier a set of \emph{labeled training data}. After the parameters are determined, we can evaluate the performance of the classifier by using it on \emph{test data}, which are unlabeled. While there are a number of commonly-used classifiers to choose from, for our study, we chose the Gradient Boosted Decision Tree classifier \footnote{For a description of this classifier and a good review of statistical learning in general, see \citep{Hastie2011}.}, since it performs quite well `out-of-the-box', with minimal tuning. This classifier is actually an example of an \emph{ensemble method} --- it combines the results from a large number of weak classifiers to assign scores to events. This combination is much more resistant to \emph{overtraining}, that is, the combined classifier will not be too specialized to the training set and can perform well on test sets as well.


\section{Supervised learning}
An event can be characterized by a vector of features $\vec{x}$ and a label $y$ that categorizes it as a signal or a background event. The task of a classifier is to predict the category \emph{y} of an event given the vector of its input features, $\vec{x}$. Thus, the classifier can simply be viewed as a function.
\section{Boosted Decision Trees}
\Adarsh{This section to be completed soon.}
The algorithm that we implement k
\begin{algorithm}
\caption{The AdaBoost.M1 Algorithm}\label{adaboost}
\begin{algorithmic}[0]
\For{m = 1 to \emph{M}}
\State Fit a classifier
\State Compute error 
\[\text{err}_m = \frac{\sum_{i = 1}^N w_i I(y_i\neq G_m(x_i))}{\sum_{i=1}^N w_i}\]
\State Compute $\alpha_m = \log[(1-\text{err}_m/\text{err}_m)]$
\State Set $w_i\gets w_i\cdot \exp[\alpha_m\cdot I(y_i\neq G_m(x_i))], i = 1,2,...N$
\EndFor
\State Output $G(x) = \text{sign}\left[\sum_{m=1}^M\alpha_mG_m(x)\right]$
\end{algorithmic}
\end{algorithm}

\section{Implementation details}
