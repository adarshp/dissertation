\chapter{Collider Phenomenology and Machine Learning}\label{ch:ColliderPheno}
In this chapter, we will discuss how to connect theory and experiment. We will begin by connecting scattering amplitudes and physically observed cross sections, after which we will briefly discuss detector components of the LHC, and then move on to the question of how we establish statistical significance given experimental data from detectors.

% Scattering amplitudes and cross sections
To observe physics at subatomic length scales, it is necessary to produce interactions on those scales. This requires colliding particles at speeds close to the speed of light. The energy of these collisions is so high that not only do the colliding particles scatter off of each other, but new particles can be created as well. 
The physical observable at particle colliders is a quantity known as the scattering cross section, denoted by $\sigma$. It is defined by the relation
\[R = \sigma\mathcal{L}\]
where \emph{R} is the rate of collisions per unit time, and $\mathcal{L}$ is the luminosity of the collider, that is, the number of particles passing through some cross-sectional area per unit time. The scattering cross-section $\sigma$ for a generic process with particles 1 and 2 colliding to produce a set of particles \emph{X} as the final state is given by the integral of the scattering amplitude for the process, $|\mathcal{M}_{12\rightarrow X}|^2$ over the phase space of \emph{X}\footnote{Here we omit the form factors that would come into play at a hadronic collider, without affecting our narrative.}. 
\begin{equation}
  \sigma_{12} = \int d\Pi_X |\mathcal{M}_{12\rightarrow X}|^2
\end{equation}
The dynamics of the collision event are contained in the matrix element $\mathcal{M}$, which will be influenced by the presence of BSM physics.
Thus, the typical quantities of interest are the \emph{differential} cross sections, $d\sigma/dx$, where \emph{x} represents some kinematic variable. 
Particle colliders are broadly categorized as either lepton or hadron colliders. Each one has its advantages and disadvantages. Lepton colliders have cleaner signals than hadron colliders due to the lack of large QCD backgrounds. On the other hand, hadron colliders are able to reach much larger center-of-mass energies, since leptons lose large amounts of energy via synchrotron radiation when forced along a circular path. In this work, we will focus on the phenomenology of hadron colliders, both present and future.
\section{Detector design for hadron colliders}
Particle detectors can be thought of as sophisticated videocameras with extremely high framerates. Conversely, the digital cameras that we use in our daily lives can be thought of as particle detectors, except designed for only one kind of particle, the photon. The response of a detector to an incident particle can take on a variety of forms. In a gaseous detector such as a  Geiger-Muller tube, an energetic incident particle will lead to the ionization of a large fraction of the gas molecules, followed by rapid recombination. This is manifested as an electrical pulse. The analogue of ionization for a solid-state detector (such as the ones used in regular digital cameras) is the creation of a large number of electron-hole pairs. The design of a particle detector will be based upon the particles it aims to detect as well as the desired precision and accuracy.
At the Large Hadron Collider at CERN, the two major detectors are ATLAS (A Toroidal LHC Apparatus) and CMS (Compact Muon Solenoid). They differ slightly in construction, but essentially probe the same physics. The components common to them are the following.
\paragraph{Tracking chamber}
The innermost part of the detector is the tracking chamber. It consists of layers of solid-state detectors that can accurately measure the paths of charged particles that are formed in the particle collision events. The presence of a strong magnetic field bends the paths of these particles, enabling us to learn about their charge and mass.
\paragraph{Calorimeter(s)}
If a particle is energetic enough to go beyond the tracking chamber, it enters the calorimeter region. A calorimeter consists of materials dense enough to completely absorb the energy of an incident particle and stop it in its tracks. At ATLAS and CMS, the calorimeter is actually a combination of two layers that are designed to stop different kinds of particles. The electromagnetic calorimeter is designed to measure the energy of electrons and photons, while the hadronic calorimeter is designed to stop (and you might have guessed this already) hadrons. 
\paragraph{Muon Detectors} The muon, being about 200 times heavier than the electron, experiences much less energy loss through bremsstrahlung, and is able to bypass both the tracking chamber and calorimeter layers. For this reason, muon detectors form the outermost layer of a particle detector.

A transverse slice of the CMS detector is shown in \autoref{fig:CMS_slice}, depicting the trajectories taken by different kinds of particles through the layers of the detector.
The specific design of a detector depends upon the vision of the collaboration running it. With a finite construction and operation budget, tradeoffs must inevitably be made. Although ATLAS and CMS are both state of the art multipurpose detectors, they have their strengths and weaknesses relative to each other based upon different sets of priorities. For this reason, we do not provide the specifics of their construction, and instead point the reader to \citep{Froidevaux2006} for a detailed review and comparison of the two.
For our purposes, the features of the collision events that we use to perform our analyses (particle momentum, missing transverse energy, etc.) can be measured with either of these detectors.
\begin{figure}[h]
  \begin{sidecaption}
    {Transverse slice of the CMS detector, showing the paths of various particles. Source: \citep{CMS_Slice}}
    \centering
    \includegraphics[trim={0 0 0 4cm},clip,width=\textwidth]{images/CMS_slice}
  \end{sidecaption}
  \label{fig:CMS_slice}
\end{figure}
\paragraph{Trigger} As mentioned in \autoref{ch:introduction}, there are hundreds of millions of collisions every second at the LHC. Recording and analyzing all of these events would be impractical, given that most of the collisions simply involve small deflections. What we are really interested in are the \emph{hard} collisions, with the final state particles having a high amount of momentum in the transverse plane. To filter out the uninteresting events, we employ a \emph{trigger} - that is, a condition that an event must satisfy to be stored for further analysis. For example, in \autoref{ch:DM_100_TeV}, we choose to trigger on a hard lepton, that is, one with a high transverse momentum.
\paragraph{Invisibles} An important point to note is that certain particles, such as neutrinos and dark matter candidates, escape the detector entirely, leaving no tracks or energy deposits. The existence of one of these `invisible' particles in a collision event can only be inferred from an observed imbalance of momenta of the final state particles in the transverse direction. Thus, for analyses involving neutrinos or dark matter, the kinematic quantity known as \emph{missing transverse energy}, denoted by $\slashed{E}_T$, is of utmost importance.

\section{Anatomy of a collider analysis}
At its heart, the goal of a collider analysis is to compare the predictions of the SM and BSM theories with actual experimental data, and estimate the level of compatibility between them. To do this well, we need precise theoretical predictions of the differential cross sections that we can expect to see at the collider. This is done with the help of Monte Carlo methods and detector simulations, as described below. 
\subsection{Parton-level event generation}
As mentioned earlier, the cross section for a scattering process is an integral of the scattering amplitude over the phase space of the final state particles. 
In general, these integrals do not have a closed form solution, and so we must resort to numerical integration. The simplest Monte Carlo integration method, the \emph{acceptance-rejection} method, involves randomly sampling points within the limits of integration with a uniform probability distribution, and testing whether they lie under the `curve' specified by the integrand. The fraction of points that pass this test, multiplied by the volume of this `bounding box' specified by the integration limits, gives the definite integral. This method is difficult to view in multiple dimensions, but not too hard to grasp in one or two dimensions - see \citep{Pyarelal2011} for a brief overview and implementation. However, drawing the random samples from a uniform probability distribution is not the most efficient method for performing Monte Carlo integrals - programs such MadGraph5 and MadEvent \citep{Alwall2014} do this in more sophisticated ways, determining the roughly most important regions of phase space and concentrating the sampling within those regions. These `points' are simply the particle collision events themselves, with coordinates given by the four-momenta of the final state particles. 
\subsection{Showering and hadronization}
At hadron colliders, dealing with the matrix element $\mathcal{M}$ alone will not suffice. We must also take into account nonperturbative QCD effects, such as the radiation of soft gluons, and the formation of complex hadronic final states. For example, even if an energetic quark is contained in an event generated by MadEvent, it will not be detected as an elementary particle. It will radiate gluons that themselves split to form new quarks, that subsequently form bound states. This collection of hadronic bound states is termed a jet, and has a momentum collinear with the momentum of the original quark. The identity of this quark can be determined (with a finite efficiency) from the properties of the jet. To handle these non-perturbative effects, we interface MadEvent with the program Pythia\citep{Sjostrand2006} which performs the steps of parton showering and hadronization.
\subsection{Detector simulation and reconstruction}
A full collider analysis carried out by experimentalists will involve detailed simulations of the detector response, using programs such as GEANT4 \citep{Agostinelli2003}. For our purposes, however, it is enough to parameterize the detector response at a higher level - for example, specifying a fixed probability for identifying a certain particle. This is done using the Delphes 3 framework \citep{DeFavereau2014a}, which provides a way to perform a fast, modular simulation of the detector response. 

\subsection{Hypothesis testing}
After signal and background samples have been generated using Monte Carlo methods and passed through a detector simulation, they are compared to actual experimental data. Doing this enables us to do one or more of the following:
\begin{itemize}
  \item Estimate some parameter, for example the mass of a particle, or a coupling strength.
  \item Set upper limits on the rate of occurrence of a process, and translating those limits into limits on the parameter space of BSM theories.
  \item Discover a new particle
  \item Compare theoretical and experimental differential cross sections using a goodness-of-fit test.
\end{itemize}
As it turns out, all of these can be subsumed into the larger framework of \emph{hypothesis testing} \citep{Heinrich}. 
%In our case, we most interested in discovering new particles, or failing that, excluding regions of parameter space for BSM theories.
In the following section, we will address how we answer the question ``Does this experimental data imply the existence of new physics (or conversely, rule it out) ?". This discussion is adapted from the one in \citep{Cranmer2015}. 

\section{Statistical significance in particle physics}
A hypothesis is a claim about what the experimental data will look like, given some model parameters. Let us consider hypothesis testing in the context of a simple counting experiment, in the frequentist interpretation. In this experiment, we examine the subset of a dataset $\mathcal{D}$ that resides in a region of the `data space' labeled the \emph{signal region} (SR), and contains $n_{SR}$ events. Our competing hypotheses are the `background-only' and `signal plus background' hypotheses, described in \autoref{tab:hypotheses}. 
\begin{table}
  \begin{tabular}{llll}
    \toprule
    Symbol & Statistical name & Physics name & Probability model\\
    \midrule
    $H_0$ & Null & Background-only & Pois($n_{SR}|\nu_B$)\\
    $H_1$ & Alternate & Signal+Background & Pois($n_{SR}|\nu_S+\nu_B$)\\
    \bottomrule
  \end{tabular}
  \caption{The two competing hypotheses for our simple number counting example. Source: \cite{Cranmer2015}.}
  \label{tab:hypotheses}
\end{table}
The probability models associated with these hypotheses are Poisson models\footnote{The exact probabiliity distribution for counting experiments is given by the binomial frequency function. In the limit of a large number of events, this approaches the Poisson frequency function, given by
  \[\text{Pois}(n|\nu) = \nu^n\frac{e^{-\nu}}{n!}\]
  which describes the probability of observing \emph{n} events given that the mean expected number of events is $\nu$.
}. The model for the 'background-only' hypothesis is Pois$(n_{SR}|\nu_B)$, that is, the probability of obtaining $n_{SR}$ events in the signal region when $\nu_B$ events are expected from the background process. The competing hypothesis, `signal plus background', on the other hand, predicts $\nu_S+\nu_B$ events in the signal region, with $\nu_S$ events from the signal process and $\nu_B$ events from the background process. The probability that the background-only hypothesis will produce at least $n_\text{SR}$ events is given by
\[p_\text{background only} = \sum_{n=n_\text{SR}}^\infty \text{Pois}(n|\nu_B)\]
The lower this probability is, the less likely it is that the background-only hypothesis can account for the observed number of events in the signal region. This quantity is also known colloquially as the \emph{p}-value - in this case, it is the \emph{p}-value for the background-only hypothesis. The corresponding \emph{p}-value for the signal + background hypothesis is given by:
\[p_\text{signal + background} = \sum_{n=n_\text{SR}}^\infty \text{Pois}(n|\nu_S + \nu_B)\]
For phenomenology, there are usually two \emph{p}-value thresholds of interest. The first is the exclusion threshold - if $p_\text{signal + background} < 0.05$, we consider the signal + background hypothesis `excluded'. The second is the discovery criterion, $p_\text{background only} < 2.87\times 10^{-7}$. When this condition is satisfied, we choose to reject the background-only hypothesis, and claim discovery of new physics. In practice, \emph{p}-values are reported in terms of equivalent \emph{Z}-values. The \emph{Z} value corresponding to a \emph{p}-value $p_0$ is given by
\[Z = \Phi^{-1}(1-p_0)\]
where $\Phi^{-1}$ is the inverse of the cumulative distribution function of the standard normal distribution (a Gaussian distribution with mean 0 and variance 1). The equivalent \emph{Z}-values for the exclusion and discovery \emph{p}-value thresholds are 1.96 and 5, respectively. In particle physics parlance, the equivalent \emph{Z} value is known as the \emph{significance}, and is reported as $Z\sigma$.
The significance is maximized by choosing signal regions with low values of $\nu_B$.
This can be seen from the expressions for $p_\text{background only}$. Lower values of $\nu_B$ lead to lower values of $p_\text{background only}$, minimizing the probability, denoted $\alpha$, that we will wrongly reject the background-only hypothesis when it is true. This is known as a Type-I error. However, in general, there can be multiple, or even an infinite number of such regions in data space. Moreover, this criterion does not let us say anything about the signal + background hypothesis. Thus, we also require that the choice of the signal region leads to a low probability (denoted $\beta$) of wrongly accepting the background-only hypothesis when the signal + background hypothesis is true (a Type-II error).

In the language of hypothesis testing, then, the job of a collider phenomenologist like myself is to find regions of data space that \emph{minimize} the probability of wrongly accepting the null hypothesis when the alternate hypothesis is true, given a \emph{fixed} probability of wrongly rejecting the null hypothesis when the alternate hypothesis is true.

Whew, that is a mouthful! In more concise terms, we would like to minimize $\beta$ for a given value of $\alpha$. The quantity $\alpha$ is known as the \emph{size} of the test, and the quantity $1-\beta$ is known as the \emph{power} of the test. And more importantly, in practical terms, this task boils down to finding regions of the data space that are densely populated by signal events (minimizing $\beta$) and sparsely populated by background events (minimizing $\alpha$). After isolating a promising signal region, we calculate the maximum achievable value of \emph{Z}. In general, the value of \emph{Z} will be calculated differently for claiming exclusion or discovery. However, we are not performing a full collider analysis, but rather a sort of `feasibility study', for which we simply use the asymptotic formula for \emph{Z} obtained by taking limit in which the probability distributions are Gaussian, and the signal rates are much smaller than the background rates.
\[Z \approx \frac{n_S}{\sqrt{n_B}}\]
where $n_S$ and $n_B$ are the number of signal and background events observed in the signal region respectively.

Traditionally, promising signal regions are found by formulating kinematic variables that can efficiently discriminate between signal and background events. These variables are designed based on our knowledge of the kinematics of the final state particles in the signal and background events. They include variables such as invariant mass, missing transverse energy, and transverse mass. While this approach has served us well so far, the models being examined are grown increasingly complex, with large parameter spaces, and with the increase in collision energy comes a much larger rate of background event production. The boundary of the optimal signal region in data space, can potentially be highly non-linear. The traditional `cut-and-count' strategy finds the signal region by applying a series of one or two dimensional selection cuts on the data. This approach can potentially miss higher-dimensional correlations in the data space. These correlations can be found efficiently through the use of \emph{machine learning} techniques, which we discuss in \autoref{sec:MachineLearning}.

\paragraph{Reach in parameter space} As mentioned earlier, new physics theories such as Two Higgs Doublet Models can have extremely large parameter spaces. Each point of a theory's parameter space can be considered as a new physics hypothesis. In our analyses, we aim to perform hypothesis tests for a large number of points in parameter space, and ascertain which of the points, or hypotheses, can be rejected, or alternatively, discovered. Doing this, we find contours of the the equivalent \emph{Z} values in the parameter space. Typically we show only the contours for $Z = 1.96$ and $Z=5$, that is, the discovery and exclusion contours. We term the area bounded by these contours the \emph{reach} of our analysis in parameter space. 

\section{Machine learning in particle physics}\label{sec:MachineLearning}
Separating signals and backgrounds in particle physics is, in the language of machine learning, a \emph{classification} problem. One can consider particle collision events as residing in a space, where the coordinates of each event are represented by various \emph{features} of the event. This is equivalent to the `data space' discussed in the previous section. These features could be high-level ones such as invariant masses, or low-level ones, such as the momentum components of individual final state particles. What we do when we perform a cut-and-count analysis is to try and isolate a region of this space that is rich in signal events. Typically, the most straightforward approach is to define a sort of `box' in feature space using rectangular, one-dimensional cuts. However, there is no guarantee that signal events are confined to such a box --- correlations between the features can distort the distribution of events in feature space. 
\strictpagecheck
\begin{figure}
  \begin{sidecaption}{Illustration of a non-linear decision boundary in two-dimensional feature space.}
  \input{code/scatter.pgf}
\end{sidecaption}
\label{fig:nonlinear_decision_boundary}
\end{figure}
If we consider a two or three dimensional feature space, this distribution of the events is typically easy to visualize, and can often be explained through a simple kinematical relation between the features. Thus, instead of a box, we might draw a ring, or perhaps some other shape as a `lasso' around the events. This kind of delineation between signal and background events is known in machine learning terms as a \emph{decision boundary}. As we go to higher dimensional feature spaces, choosing this decision boundary by visual inspection becomes virtually impossible - instead, we resort to looking at one and two dimensional slices of the space. 

However, inspecting these lower dimensional slices may fail to fully capture the potentially complicated correlations among the features, and we might end up rejecting too many signal events, or accepting too many background events. Take for example the distribution of data from a toy experiment in \autoref{fig:nonlinear_decision_boundary}. The decision boundary that separates the signal events (blue) and the background events (red) is curved. Applying rectilinear cuts in this feature space would clearly be suboptimal. Machine learning (ML) techniques (or, as they are sometimes referred to in the particle physics literature, multivariate techniques) can harness the correlations between features more efficiently than manually attempting to find the optimal decision boundary in feature space.

In the context of particle physics, a \emph{binary classifier} is a function that takes as input various features of an event, and outputs a score for the event corresponding to whether it is more signal-like or more background-like. The form of the function itself depends on a number of parameters, which are \emph{a priori} unknown. In \emph{supervised learning} (which encompasses the most commonly-used ML techniques), these parameters are set by feeding the classifier a set of \emph{labeled training data}. After the parameters are determined, we can evaluate the performance of the classifier by using it on \emph{test data}, which are unlabeled. While there are a number of commonly-used classifiers to choose from, for our study, we chose the Gradient Boosted Decision Tree classifier \footnote{For a description of this classifier and a good review of statistical learning in general, see \citep{Hastie2011}.}, since it performs quite well `out-of-the-box', with minimal tuning. This classifier is actually an example of an \emph{ensemble method} --- it combines the results from a large number of weak classifiers to assign scores to events. This combination is much more resistant to \emph{overtraining}, that is, the combined classifier will not be too specialized to the training set and can perform well on test sets as well.

While the analysis in \autoref{ch:LightChargedHiggs} does not employ ML techniques, the analyses in chapters \ref{ch:ExoticHiggs} and \ref{ch:DM_100_TeV} do.
